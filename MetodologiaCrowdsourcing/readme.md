

##


# Herramientas y metodología crowdsourcing para la participación y creación colectiva de conocimiento abierto

### Miguel Gea Megías, Universidad de Granada, 2022

> (Versión Creative Commons del artículo contenido en libro Transmedialización y crowdsourcing en la cultura mediática contemporánea, J. Alberich D.  Sánchez-mesa (eds.),  Ed. Universidad de Granada, ISBN 978-84-338-6929-6




El cambio hacia una cultura digital está transformando radicalmente nuestra sociedad tal y como se concebía hasta nuestros días. 
La irrupción de la tecnología ha dado lugar a un cambio profundo de nuestros hábitos y la proliferación de nuevos medios. 
Analizaremos cómo ha sido la evolución de esa adopción tecnológica analizando en primer lugar la capa de hardware, la de software y 
las estrategias para la colaboración en la red. Finalmente se propondrá una estrategia para fomentar el pensamiento creativo. 

## Introducción 

> *El mundo ya no es analógico, sino digital,* y por tanto, 
>
> debemos reflexionar sobre la diferencia que existe entre bits y átomos
>
> *Being Digital, 1995* - *Nicholas Negroponte (1943- )*



Estamos en tiempos de cambio. Autores como Castells consideran que “una revolución tecnológica centrada en torno a las tecnologías de la información empezó a reconfigurar la base material de la sociedad a un ritmo acelerado” (Castells, 1997). 

La tecnología ha provocado cambios disruptivos evidentes que afectan nuestros hábitos cotidianos de comportamiento de una manera generalizada y globalizada (en mayor o menor medida) adaptándonos a los artefactos que nos rodean (smartphones, asistentes, localizadores, sensores) y a la adopción de una identidad digital. Ya no somos casi conscientes de la primera vez que usamos el correo electrónico o creamos nuestro primer perfil en una red social. 

Sin embargo, estas son rutinas recientes que se experimentaron a partir de los 90 de forma bastante artesanal y que llegaron al gran público con el comienzo del siglo XXI cuando el acceso a la red y los dispositivos de conexión se fueron abaratando y siendo más fiables. 

De los autores que han hablado sobre este proceso de cambio, quizás sea Nicholas Negroponte quien pudo vislumbrar ese cambio que se avecinaba con más nitidez dada su posición privilegiada como fundador y director del MIT Medialab, un instituto de investigación heterogéneo y centro de referencia en innovaciones tecnológicas con sinergias de colaboración en varias áreas de desarrollo. Esto le permitió conocer de primera mano y vislumbrar el potencial de investigaciones emergentes que rápidamente saldrían del laboratorio a la sociedad, y que plasma en su libro *Ser Digital* vaticinando que habrá que “saber la diferencia entre bits y átomos” (Negroponte, 1995). En ese libro, presagio de los muchos cambios que acontecieron en el inicio del siglo XXI, habla de la paradoja del escribir en formato libro (cuando en esas fechas el avance de uso de tecnología digital multimedia es exponencial) ya que está pensado para las personas que todavía no han dado el todavía el paso a digital, y usa el poder de la palabra escrita porque *suelta destellos de imágenes y evoca metáforas que adquieren significado a partir de la imaginación y de las propias experiencias del lector* (Negroponte, 1995). 

También nos habla de la transformación de los átomos en bits y la transformación socioeconómica que eso implica, como por ejemplo, “los primeros átomos de entretenimiento que se sustituirán por bits son los videos de alquiler” (Negroponte, 1995). Incidiendo en esa idea, plantea que será un cambio generalizado, en ese proceso de digitalizar la información “transformando todos los medios en bits”, tendrá como consecuencia la mezcla y combinación inmediata de todos esos tipos de contenidos. En su libro habla de la multimedia, concepto que posteriormente otros autores extienden hacia la remediación e hibridación 
Ahora, más de dos décadas después y con una pandemia mundial por medio que ha aislado a las personas en sus casas durante meses, vemos cómo los vídeos bajo demanda y el streaming son el medio habitual de ocio en las familias. Los libros siguen teniendo su espacio, pero ya existen dispositivos para almacenarlos y leerlos en cualquier lugar, o bien aplicaciones que leen por tí y te permiten “escucharlo”. 

Esta transformación vertiginosa (y a veces traumática) nos ha situado en un contexto (nunca imaginado en la historia de la humanidad) de acceso a la información al instante, (hiper-)comunicación en tiempo real, asistentes de todo tipo para toma de decisiones y una inimaginable cantidad de nuevas posibilidades que supera nuestra imaginación, y todo ello a través de un acceso tan sencillo y simple como es un teléfono móvil y conexión a la *red*. 

Este contexto también provoca un cambio de actitud donde los “usuarios” se convierten en una parte muy importante de ese espacio virtual. En esta evolución, ya no basta con participar con un “me gusta” o “ser tu amigo” sino que podemos dar un paso adelante hacia la construcción de espacio de creatividad y participación 

## Los cimientos del mundo digital: el hardware

> *Las computadoras no sólo deben servir para ser eficientes y productivas,*
>
> *sino que deben contribuir a aumentar la capacidad intelectual`*
>
> (Douglas Engelbart, 1925-2013)*

Para comprender esta sociedad de la interacción, debemos ubicarnos en el concepto de ordenador (o computadora) como piedra filosofal donde ha girado todo este proceso de transformación. En este sentido, la innovación necesaria para generar estos avances se debe apoyar en una serie de acontecimientos donde las ideas surgen cuando existe la tecnología adecuada para ponerla en práctica (Isaacson, 2014). De este modo, podemos hacer un rápido recorrido por los avances más importantes: la primera calculadora analógica (1930) por Vannevar Bush o la máquina de Turing (1936) por el matemático inglés Alan Turing que plantea el concepto teórico de algoritmo y programable. 

En poco tiempo (década de los 40) surgen los primeros ordenadores con características que podamos equiparar a los actuales (digitales, memoria, procesamiento, almacenamiento) como ENIAC (1946). En esta primera etapa nos encontramos con máquinas inmensas que deben ser manipuladas por un equipo de técnicos y pensadas para su uso por científicos (matemáticos, físicos) que les dan un uso funcional para resolución numérica.

El problema del tamaño, su coste y la relativa complejidad para su programación son el motivo de este entorno cerrado sólo para científicos, que se extendería hasta la década de los 80 con la aparición de los microprocesadores. 
Sin embargo, eso no era impedimento para que surgieran los pioneros con incursiones en prácticas artísticas creativas generadas por ordenador. Autores como Manfred Mohr, Michael Noll o George Ness entre otros experimentan con la generación de dibujos programados mediante técnicas de *modularidad*, *recurrencia* y *aleatoriedad* (Franke, 1985). En la Figura 1 podemos ver una obra de G. Ness y a su derecha, el código de programa (escrito en Processing) con el tutorial explicando el proceso (http://www.artsnova.com/Nees_Schotter_Tutorial.html)  

![img](https://lh4.googleusercontent.com/A7MqbXNVaz2EybhkSGszPh3UG9xBVjyE-gvU2s3qNIkbSGF7f8mFiHrAjUM1Q_r9Dv6St7wGwfLyyHFWrb6ZHT6CZEM6PbOPjcMF02Le5N2uUBgPDDXXzvEzE6QptyMi1LBzaRU)


~~~
// Georg Nees Schotter 
// Reproduction by Jim Plaxco, [www.artsnova.com](www.artsnova.com)

int columns = 12;    // number of columns of squares
int rows = 22;     // number of rows of squares
int sqrsize=30;     // size of each square
float rndStep=.22;   // Rotation Increment in degrees 
float randsum=0;    // Cumulative rotation value 
int padding=2*sqrsize; // margin area
float randval;     // random value for translation and rotation
float dampen=0.45;   // soften random effect for translation 

void setup() {  
       size((columns+4)*sqrsize,(rows+4)*sqrsize);  
       background(255);  // set background color to white 
       stroke(0);     // set pen color to black 
       smooth();     // use line smoothing  
       noFill();     // do not fill the squares with color 
       rectMode(CENTER); // use x,y value as the center of the square 
       noLoop();     // execute draw() just one time
       } // end of setup() 
       
 void draw() {
    for (int y=1; y <= rows; y++) {  
    randsum += (y*rndStep); // Increment the random value  
    for (int x=1; x <= columns; x++) {   
          pushMatrix();   
          randval = random(-randsum,randsum);  
          translate(padding + (x * sqrsize) - (.5*sqrsize) + (randval*dampen),padding +
                        (y * sqrsize) - (.5*sqrsize) + (randval*dampen));
          rotate(radians(randval));   
          rect(0,0,sqrsize,sqrsize);   
          popMatrix();  
          } // end of x loop 
      } // end of y loop
  } // end of draw()  
~~~
Figura 1. Schotter (Georg Nees, 1965) y su código equivalente en Processing


De ese modo, las limitaciones inherentes a esas primeras máquinas computacionales abrió paso a una creatividad basada en modelos matemáticos para crear los primeros algoritmos que mediante funciones recurrentes representaban los primeros fractales, o el uso de sucesiones de fibonacci para crear bellas representaciones visuales en toscos aparatos de representación. Se puede visitar estas obras y conocer los artistas en el Museo de Arte Digital ([DAM]( https://dam.org/)), proyecto surgido en Alemania y que preserva obras de estos pioneros. 

El concepto de *ordenador personal* ya fue concebido por Vannevar Bush en 1945 en el artículo “As We May Think” 
donde habla en los siguientes términos: 
> *“Consider a future device… in which an individual stores all his books, records, and communications, * 
> 
> *and which is mechanized so that it may be consulted with exceeding speed and flexibility.* 
> 
> *It is an enlarged intimate supplement to his memory”* (Bush, 1945).


Esa idea conceptual se enriqueció con otros avances que jóvenes estudiantes entusiastas como el caso de Ivan E. Sutherland, que en 1963 presenta *Sketchpad* un prototipo que permite diseñar imágenes interactivas en una pantalla usando un lápiz óptico. 

Este hito se complementa con la invención por parte de Douglas Engelbart del ratón (1965) que supuso todo un acontecimiento y del cual se tiene constancia todavía con una demostración que causó gran expectación en el mundo científico y que por nada del mundo se quiso perder Alan Kay, precursor de la arquitectura de los ordenadores contemporáneos. La presentación del ratón en 1968 fue denominada “The Mother of All Demos” y se encuentra disponible para su visualización en Youtube


Sin embargo, es con la aparición del PC (el ordenador personal) en la década de los 80 cuando hay una transformación radical en paradigma de la cultura digital: se busca al *usuario universal*, cualquier persona puede usar este medio para su trabajo, el ocio, o incluso puede “programar” sus propias aplicaciones al estilo de los primeros pioneros. Se crean las primeras aplicaciones universales que pueden ayudar en cualquier tarea habitual: un procesador de textos, una hoja de cálculo, un programa de diseño y algunas utilidades para la organización (agenda, recordatorios) y gestión de los datos (almacenamiento en carpetas y capacidad de movilidad mediante soportes magnéticos). Incluso en esa época un joven Bill Gates se atrevía a aventurar que en un futuro próximo (en 25 años) habrá uno de esos (PCs) en todas las casas. La realidad ha ido mucho más allá de esa predicción, ya estamos rodeados de dispositivos inteligentes y el concepto de ordenador personal ha quedado relegado en un segundo plano. 
Sin embargo, es interesante comprobar la reacción y admiración de las personas cuando accedían por primera vez a esos modernos centros de trabajo y ocio. En esos tiempos (e incluso ahora), la vida de una persona estaba llena de novedades increíbles. Lev Manovich cuenta esa evolución como parte de la historia de su vida, cuando en 1981 se traslada de Moscú a Nueva York en 2001, y aunque ya conocía algunos principios de la programación y uso de los medios, “su primera experiencia con gráficos fue en 1983-84 con un Apple IIe [..] y en 1984 vio la implantación exitosa de la primera interfaz gráfica de usuario (GUI) en Apple Macintosh” (Manovich, 2013)
Esa rivalidad MAC-PC, o lo que es lo mismo, Jobs-Gates, fue un magnífico caldo de cultivo para las continuas innovaciones y fomento de la creatividad en el mercado de la tecnología al servicio del usuario común. En el imaginario colectivo queda la presentación del nuevo Macintosh en un anuncio dirigido por Ridley Scott que fue emitido por primera vez durante la SuperBowl de 1984. Hay que recordar que ambos sistemas operativos, la base software para el funcionamiento de un ordenador, fueron herederos de sistema Unix y X-Windows, un hito que surgió como concepto en las instalaciones de Xerox Parc, un centro de investigación puntero donde se establecieron las primeras pautas para crear la metáfora *desktop* WIMP basada en los conceptos de ventana (contenedor de información), iconos, menús y uso del puntero (ratón). Es en Xerox Parc donde Alan Kay desarrolló estos conceptos que hoy en día siguen vigentes, siendo además uno de los artífices del desarrollo de *Xerox Star,* el primer ordenador con representación gráfica (Kay, 1990) e inventor de *Dynabook*, el precursor de la tablet actual (Kay, 1977). Toda esta transformación tiene un punto de inflexión muy importante con la aparición de las redes de comunicaciones con ARPANET, un conglomerado de sistemas de conexión por cable con un propósito inicialmente militar, pero que rápidamente se transformó en los protocolos y arquitectura que hoy conocemos como Internet**.** Una de las aportaciones más valoradas ha sido la invención por parte de Tim Berners-Lee de la *Web (World wide web)* como sistema de información universal mientras trabajaba como becario en el CERN, que era un centro donde él mismo comentaba: “parecía un micromundo de investigadores de diferentes procedencias e idiomas que necesitaban compartir información, se buscaba un sistema que permitiese a la gente poner en común sus pareceres y realizar el seguimiento de la memoria institucional del proyecto” (Isaacson, 2014). El concepto de *hipertexto* fue la clave para crear esos documentos que se codifican de tal manera que al seleccionar un elemento, te redirige a otro documento o fragmento de contenido. Quedará para la historia el informe que Tim Berners-Lee entregó a su jefe entonces (Mike Sendall) en 1989 con las ideas de Esquirre, el precursor de lo que sería el concepto de la web un año más tarde, y donde su jefe escribió las palabra que pasarían a la historia: “vague but exciting”. 
Este recorrido por los hitos y nombres relevantes del siglo XX muestra los orígenes de lo que actualmente son las bases de nuestro mundo digital: esos elementos tecnológicos que nos rodean conforman lo que Weiser denomina *tecnologías profundas*, esto es, “las tecnologías profundas son las que *desaparecen*, se entretejen y se fabrican en la vida cotidiana hasta que son indistinguibles” (Weiser, 1991). Nos acostumbramos de tal modo a su existencia y manipulación que su uso es transparente y natural (conectar la wifi, subir documentos a la nube, loguearse, spam, etc.)  
Esos logros no hubieran sido posible sin la necesaria participación de muchos pioneros en un espacio como Sillicon Valley, donde unos aprendían de otros y se creaba un espacio competitivo pero también de aprendizaje mutuo: Jobs y Gates rivalizaban a nivel comercial pero ambos compartían estrategias para incluir software de Microsoft en los Apple, mientras que Alan Kay estaba al tanto de los avances de Engelbart, e incluso asistió a su famosa demostración. Además, existía una cultura de emprendimiento que provocaba constantes cambios de empresas, se vivía con una filosofía de trabajo que prefería “lanzarse a fundar tu propia compañía y fracasar, que permanecer durante treinta años en la misma empresa” (Isaacson, 2014). Esto sin duda contribuyó de manera muy notable a la proliferación de ideas que eran compartidas libremente. 
**3. La cibercultura y los nuevos medios: el auge del software** 

*Vivimos en dos mundos paralelos y diferentes: el online y el offline.* *Hemos llegado a un punto en el que pasamos más tiempo frente a pantallas que frente a otras personas y eso tiene efectos perturbadores que no solemos percibir”*               *(Zygmunt Bauman, 1925-2017)* 
En todo este proceso de cambio, tan importante como el hardware era el software: esas piezas de código que permitían poner en funcionamiento esas maquinarias, y de entre todas esas piezas, el *sistema operativo* era el pilar donde poder construir el resto de la arquitectura lógica. Ya se ha comentado la rivalidad lograr la primacía en el mercado entre Mac/PC, y parte de esa estrategia se basaba en el desarrollo de *sistemas operativos* (MacOS/Windows) que fuesen lo más atractivos posibles, robustos, funcionales pero fáciles de usar para atraer a la comunidad de programadores, profesionales del sector pero también, al pequeño y medio usuario que normalmente no tiene unos conocimientos amplios de tecnología. 
Sin embargo, esta historia comenzó mucho antes, en 1969, cuando el mercado de sistemas operativos estaba liderado por UNIX (desarrollado en los laboratorios Bell de AT&T) que mantenía su liderazgo en en el sector profesional por su robustez y fiabilidad, si bien su uso suponía un alto coste económico para las empresas. La necesidad de experimentar y hacer mejoras (más allá de las que se realizaban por los propios empleados) hizo que se hicieran algunas concesiones a universidades como el desarrollo de la versión BSD (Berkeley Software Distribution) de UNIX en 1975. Sin embargo, ese entorno era tan restrictivo en cuanto a pago de licencias y control de uso que provocó de forma espontánea un nuevo tipo de estrategia de creación de software por miembros del laboratorio de Inteligencia Artificial del MIT que se autodenominan *hackers* (por hacer bromas y trucos entre ellos). Entre ellos destacaba Richard Stallman, enamorado de la cultura de compartir código y que tenía la intención de crear su propio Sistema Operativo similar a UNIX pero que estuviese disponible de forma libre. En 1983 funda el proyecto GNU (acrónimo de GNU No es UNIX) e introduce el concepto de *copyleft* (en contraposición a la costumbre de crear con licencia copyright). Con esta idea crea la FSF (Free Software Foundation) con la filosofía de que cualquier producto creado bajo la Fundación pudiera ser utilizado, modificado y con la condición de difundir las modificaciones que se llegasen a hacer al programa (Isaacson, 2014). No hay duda que Stallman es el impulsor del término *software libre,* y en su manifiesto GNU deja claro el concepto de recompensa para el programador: “Si hay algo que merece una recompensa, es la contribución social. La creatividad puede ser una contribución social, pero solo en la medida en que la sociedad sea libre de aprovechar los resultados” (Stalllman, 1985)
Con esa idea de ética hacker (tal como la concebía Stallman) no consiguió completar el sueño del sistema operativo GNU, pero fue el empujón para que un joven inquieto finlandes llamado Linus Tovalds creara *Linux*, una versión clon de UNIX a partir de un producto educativo MINIX. Con sólo parte del código desarrollado en 1991 se dirige a la antigua comunidad de usuarios de MINIX para solicitarles ayuda y colaboración publicando el código. Esta iniciativa de liberar el kernel de Linux bajo licencia GNU “dio pie a un tsunami de colaboraciones voluntarias que se convirtió en el modelo para la producción comunitaria que propulsó la innovación en la era digital”. Este acontecimiento tuvo mucha repercusión en la prensa, y “el cuerpo de hackers que surgió alrededor de GNU y Linux demostró que los incentivos emocionales, más allá de la recompensa económica, pueden ser motivación suficiente para la colaboración voluntaria”. (Isaacson, 2014). 
De esa manera, la filosofía de L. Torvalds (con la inspiración de R. Stallman) sobre el software y la cultura de compartir libremente ha sido una de las contribuciones más importantes a la cultura digital contemporánea. Linux era un proyecto colectivo, descentralizado y no jerárquico, que aunaba entusiasmo por los participantes ansiosos de poder contribuir de forma altruista. 
A estos fenómenos de participación espontánea, Pierre Lèvy lo describe como *inteligencia colectiva,* esto es “la inteligencia grupal que emerge de la colaboración y competición de muchos individuos, en la que existe un saber colectivo que puede potenciarse a través de la tecnología. Esta capacidad permite colaborar a un grupo de personas (de forma consciente o no) para lograr objetivos de alta complejidad” (Lèvy, 1997). 
La tecnología, como indica Lèvy, es el catalizador de estas prácticas colectivas, y su imbricación en todas nuestras actividades cotidianas permite plantear un nuevo término: la *cibercultura* como “la cultura que surge del uso de los nuevos medios para la comunicación, entretenimiento, formación y demás actividades de la vida diaria”. Esta redefinición del concepto implica mucho más de lo que inicialmente se podría pensar, y de hecho, afecta a nuestra forma de adquirir conocimientos, ya que nos encontramos con grandes diferencias respecto al pasado. En primer lugar, la renovación del conocimiento es mucho más rápida que en otras épocas y la obsolescencia de los mismos hace que debamos estar en un proceso de reciclado permanente (Lèvy, 2000). Esto es debido en parte a que la edición (impresa) es mucho más lenta que la edición online, por lo que los nuevos conocimientos fluyen con mucha más rapidez, incluso afectando a nuestra capacidad de *procesamiento* y afloran efectos nocivos como la *sobrecarga de información* (Horrigan, 2016), la *infoxicación* (Cornella, 2000), o la *distracción*, ya que “en Internet triunfa la distracción y se desincentiva el pensamiento profundo” (Carr, 2011). 
En segundo lugar, ya no sólo es importante adquirir nuevos conocimientos, sino que también cobra especial importancia la transmisión de los mismos: cada vez es más frecuente que el trabajo consista en transmitir conocimientos, y en ese sentido el *ciberespacio* (concepto asociado a los medios digitales asociados a esa nueva cibercultura) cobra cada vez más relevancia (Lèvy, 2000). Podemos destacar el trabajo de Jimmy Wales, que en 2001 crea la *wikipedia*, un proyecto encaminado a la creación de una enciclopedia libre mediante aportaciones altruistas en las que cualquier persona pueda participar. Esta iniciativa tiene muchos puntos en común con con el concepto de *la memoria colectiva* acuñado por Halbwachs: “la memoria colectiva es un fenómeno que comparte un grupo y que se desprende de los hechos y experiencias que conciernen a la mayor parte de sus miembros resultado de sus experiencias propias o de las relaciones con los individuos más próximos con los que tienen más contacto” (Halbwach, 1980). La wikipedia representa un claro ejemplo de memoria colectiva que (con más o menos conflictos) se tiene sobre los hechos y conocimientos que acontecen en el mundo en un instante dado, y debido a la capacidad de edición de la propia herramienta, que pueden cambiar con el tiempo (añadiendo, modificando o eliminando partes que se hayan consensuado por la comunidad). 
La consolidación de la Web fue con la nueva denominación de *Web 2.0 que* ya era toda una declaración de intenciones. En la definición de 2004 por Tim O’Reilly hablaba de (entre otras cosas) la World Wide Web como plataforma de trabajo, el fortalecimiento de la inteligencia colectiva, el software no limitado a un solo dispositivo y ricas experiencias enriquecedoras de los usuarios (la web participativa). De ahí surge el universo de nuevas experiencias en el ciberespacio que actualmente conocemos: la eclosión de las redes sociales para fortalecer nuestras relaciones, la mensajería instantánea para una comunicación ágil en tiempo real, los espacios personales de comunicación (blogs, podcast, wikis, newsletter), o de transmisión (*streaming*) en directo (Cobo, Pardo, 2007). Fue tal el éxito de esa nueva aproximación que la revista Time declaró en 2006 como personaje del año a tí (*you*), reflejando la idea que *tú* (cada persona anónima) está transformando la era de la información, tu participación hace que la red cobre vida y sentido. 
Este es un punto de no retroceso y es donde ya podemos hablar del nuevo usuario de la red: *los prosumidores*, esto es, usuarios bidireccionales que dejan de ser meros consumidores (de información y conocimiento) de actitud pasiva y pasan a ser productores activos (de ideas, conocimiento). Este es un concepto que fue acuñado originariamente de una novela de ciencia ficción: la tercera ola (Toffler, 1980) y que ha sido un elemento fundamental para estudiar el comportamiento de los usuarios en el ciberespacio. Esta bidireccionalidad provoca un nuevo efecto colateral: el estudio de los nuevos medios con los que nos comunicamos en este espacio digital. 
Antes de nada, cabe entonces preguntarse cuándo los *medios clásicos* (tales como como la fotografía o el cine) se convirtieron en *nuevos medios*. Lev Manovich en lugar de catalogarlos de forma exhaustiva, nos da una serie de principios que deben poseer los nuevos medios: la representación numérica, la modularidad, la automatización, la variabilidad y la transcodificación (Manovich, 2001). Esa caracterización nos permite profundizar en las habilidades de cada medio y analizar sus mutaciones o hibridaciones a lo largo del tiempo. En ese contexto, el ordenador es un metamedio que puede estar contenido todo o parte de esos nuevos medios. La fotografía, por ejemplo, es una representación digital tanto en la captura (cámaras de los smartphones, cámaras digitales, escaneado de imágenes) , en su procesamiento (mediante software como photoshop, gimp) y en su representación (pantalla, hologramas 3D, realidad virtual, etc). 






## Referencias Web


* Museo de Arte Digital ([DAM]( https://dam.org/))


* [*The Mother of All Demos*](https://www.youtube.com/watch?v=yJDv-zdhzMY) (D. Engelbart, 1968) https://www.youtube.com/watch?v=yJDv-zdhzMY
